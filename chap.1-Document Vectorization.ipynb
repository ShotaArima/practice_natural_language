{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShotaArima/practice_natural_language/blob/main/chap.1-Document%20Vectorization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 第1章 文書のベクトル化"
      ],
      "metadata": {
        "id": "USEkWJ7hFUQF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## janomeによる単語分割"
      ],
      "metadata": {
        "id": "2EyHduCEFfJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install janome"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXNfbxBCEYvw",
        "outputId": "8a717d85-bda1-4b25-e6a3-1fb6b84d1503"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting janome\n",
            "  Downloading Janome-0.5.0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Downloading Janome-0.5.0-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: janome\n",
            "Successfully installed janome-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import janome\n",
        "from janome.tokenizer import Tokenizer"
      ],
      "metadata": {
        "id": "l3kB5m4B1J0J"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = Tokenizer()"
      ],
      "metadata": {
        "id": "sZvdKVKiEM4I"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in t.tokenize('私は秋田犬が大好きです。'):\n",
        "  print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8bhuXfyFm0q",
        "outputId": "508a1752-736b-45c0-acd2-2f48c6ab3385"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "私\t名詞,代名詞,一般,*,*,*,私,ワタシ,ワタシ\n",
            "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
            "秋田\t名詞,固有名詞,地域,一般,*,*,秋田,アキタ,アキタ\n",
            "犬\t名詞,一般,*,*,*,*,犬,イヌ,イヌ\n",
            "が\t助詞,格助詞,一般,*,*,*,が,ガ,ガ\n",
            "大好き\t名詞,形容動詞語幹,*,*,*,*,大好き,ダイスキ,ダイスキ\n",
            "です\t助動詞,*,*,*,特殊・デス,基本形,です,デス,デス\n",
            "。\t記号,句点,*,*,*,*,。,。,。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# wakati=Trueで分かち書きモード\n",
        "for token in t.tokenize('私は秋田犬が大好きです。', wakati=True):\n",
        "  print(token, end='/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpIEgY79F19i",
        "outputId": "05980aab-b886-425f-e4a2-d82870e91fc8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "私/は/秋田/犬/が/大好き/です/。/"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 半角スペースで分割\n",
        "for token in t.tokenize('私は秋田犬が大好きです。', wakati=True):\n",
        "  print(token, end=' ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oH0oxKabGtid",
        "outputId": "033cc69a-23d3-4bb3-f9c4-c34d6c109107"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "私 は 秋田 犬 が 大好き です 。 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# リストに格納\n",
        "words = [token for token in t.tokenize('私は秋田犬が好きです。', wakati=True)]\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pu_QQBgHU2y",
        "outputId": "bdfc057d-e23f-487d-ed92-d9bb0e30daae"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['私', 'は', '秋田', '犬', 'が', '好き', 'です', '。']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## N-gram"
      ],
      "metadata": {
        "id": "-UVxhpBNPDNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# bi-gram\n",
        "for i in range (len(words)-1):\n",
        "    print(f\"{i}:{words[i:i+2]}\")"
      ],
      "metadata": {
        "id": "3fBbBogOH01a",
        "outputId": "feeeea02-b84a-4e76-c098-b5f421be8fe2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:['私', 'は']\n",
            "1:['は', '秋田']\n",
            "2:['秋田', '犬']\n",
            "3:['犬', 'が']\n",
            "4:['が', '好き']\n",
            "5:['好き', 'です']\n",
            "6:['です', '。']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# n-gram\n",
        "def get_word_n_grams(sentence, n):\n",
        "    words = [token for token in t.tokenize(sentence, wakati=True)]\n",
        "    result = []\n",
        "    for index in range(len(words)):\n",
        "        result.append(words[index: index+n])\n",
        "        if index+n >= len(words):\n",
        "            return result"
      ],
      "metadata": {
        "id": "BpT7FtHwPMT6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = 'すもももももももものうち'\n",
        "print(get_word_n_grams(input, 3))"
      ],
      "metadata": {
        "id": "GyUk0jJzQh5z",
        "outputId": "05dddd8e-645c-4233-92d4-b197cbe7ab15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['すもも', 'も', 'もも'], ['も', 'もも', 'も'], ['もも', 'も', 'もも'], ['も', 'もも', 'の'], ['もも', 'の', 'うち']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 文字のN-gram"
      ],
      "metadata": {
        "id": "a1zRT4x0RvcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_charactor_n_gram(sentence, n):\n",
        "    result = []\n",
        "    for index in range(len(sentence)):\n",
        "        result.append(sentence[index: index+n])\n",
        "        if index+n >= len(sentence):\n",
        "            return result"
      ],
      "metadata": {
        "id": "MV9K0UYJQvLs"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input2 = '私は秋田犬が大好きだ。'\n",
        "print(get_charactor_n_gram(input2, 2))"
      ],
      "metadata": {
        "id": "8j3A4EYKSJSU",
        "outputId": "a2c7b38b-fc8f-4bd7-e800-529217653ba6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['私は', 'は秋', '秋田', '田犬', '犬が', 'が大', '大好', '好き', 'きだ', 'だ。']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag-of-words"
      ],
      "metadata": {
        "id": "FBXV9I0jnH5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(sentence):\n",
        "    return [token for token in t.tokenize(sentence, wakati=True)]"
      ],
      "metadata": {
        "id": "Lwr1276ESXwk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words1 = '私は秋田犬が大好き。'\n",
        "words2 = '私は犬が少し苦手。'\n",
        "\n",
        "print(f\"words1:{tokenize(words1)}\")\n",
        "print(f\"words2:{tokenize(words2)}\")"
      ],
      "metadata": {
        "id": "aUH18EaMnUef",
        "outputId": "45213420-60a4-4cfd-d7c0-565d98baff9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "words1:['私', 'は', '秋田', '犬', 'が', '大好き', '。']\n",
            "words2:['私', 'は', '犬', 'が', '少し', '苦手', '。']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 文書ベクトルの作成\n",
        "def get_bag_of_words(document):\n",
        "    result_dict = {}\n",
        "    words = tokenize(document)\n",
        "    for word in words:\n",
        "        if word not in result_dict:\n",
        "            result_dict[word] = 1\n",
        "        else:\n",
        "            result_dict[word] += 1\n",
        "    return result_dict"
      ],
      "metadata": {
        "id": "FUv5S_J2ntwo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document1 = '私は秋田犬が大好き。秋田犬は私が大好き。'\n",
        "document2 = '私は犬が少し苦手。'\n",
        "print(f\"document1:{get_bag_of_words(document1)}\")\n",
        "print(f\"document2:{get_bag_of_words(document2)}\")"
      ],
      "metadata": {
        "id": "y3WKy5ZEppGb",
        "outputId": "641d2880-af48-47d0-d07d-b6d5240f6f5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "document1:{'私': 2, 'は': 2, '秋田': 2, '犬': 2, 'が': 2, '大好き': 2, '。': 2}\n",
            "document2:{'私': 1, 'は': 1, '犬': 1, 'が': 1, '少し': 1, '苦手': 1, '。': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 辞書\n",
        "def make_dictinary(documents):\n",
        "    result_dict = {}\n",
        "    index = 1\n",
        "    for doc in documents:\n",
        "        words = tokenize(doc)\n",
        "        for word in words:\n",
        "            if word not in result_dict:\n",
        "                result_dict[word] = index\n",
        "                index+=1\n",
        "    return result_dict"
      ],
      "metadata": {
        "id": "0P316AEHp_I5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [document1, document2]\n",
        "dictionary = make_dictinary(documents)\n",
        "print(f\"dict:{dict}\")"
      ],
      "metadata": {
        "id": "jFeG0ZEisI14",
        "outputId": "fa9b8345-d92b-4e79-bbf0-ecc2951c38c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict:<class 'dict'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 用例ベクトルの作成\n",
        "def make_BOW_vectors(documents, dictionary):\n",
        "    result_vectors = []\n",
        "    for doc in documents:\n",
        "        vec = {}\n",
        "        words = tokenize(doc)\n",
        "        for entry in dictionary:\n",
        "            vec[dictionary[entry]]=0\n",
        "        for word in words:\n",
        "            vec[dictionary[word]] += 1\n",
        "        result_vectors.append(vec)\n",
        "    return result_vectors"
      ],
      "metadata": {
        "id": "xu71RG3psaZh"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectors = make_BOW_vectors(documents, dictionary)\n",
        "print(f\"結果:{vectors}\")\n",
        "\n",
        "id_to_word = {v: k for k, v in dictionary.items()} # 辞書型を逆変換する\n",
        "word_occurrences = []\n",
        "for occurrence in vectors:\n",
        "    word_dict = {}\n",
        "    for id_num, count in occurrence.items():\n",
        "        word = id_to_word[id_num]\n",
        "        word_dict[word] = count\n",
        "    word_occurrences.append(word_dict)\n",
        "for occurrences in word_occurrences:\n",
        "    print(f\"結果:{occurrences}\")"
      ],
      "metadata": {
        "id": "k50KEnwMtufs",
        "outputId": "69445f8f-d13c-488f-dd86-6726f74b7108",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "結果:[{1: 2, 2: 2, 3: 2, 4: 2, 5: 2, 6: 2, 7: 2, 8: 0, 9: 0}, {1: 1, 2: 1, 3: 0, 4: 1, 5: 1, 6: 0, 7: 1, 8: 1, 9: 1}]\n",
            "結果:{'私': 2, 'は': 2, '秋田': 2, '犬': 2, 'が': 2, '大好き': 2, '。': 2, '少し': 0, '苦手': 0}\n",
            "結果:{'私': 1, 'は': 1, '秋田': 0, '犬': 1, 'が': 1, '大好き': 0, '。': 1, '少し': 1, '苦手': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CountVectorizerモジュール\n",
        "- scikit-learnのモジュール"
      ],
      "metadata": {
        "id": "K2FIpU4ZQqQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# wakatiを使用して単語間に半角スペースを入れる\n",
        "def make_corps(documents):\n",
        "    result_corps = []\n",
        "    for doc in documents:\n",
        "        words = tokenize(doc)\n",
        "        text=\" \".join(words)\n",
        "        result_corps.append(text)\n",
        "    return result_corps"
      ],
      "metadata": {
        "id": "B-Ymke3quyuK"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t  = Tokenizer()\n",
        "document1 = '私は秋田犬が大好き。秋田犬は私が大好き。'\n",
        "document2 = '私は犬が少し苦手。'\n",
        "documents = [document1, document2]\n",
        "corpuses = make_corps(documents)\n",
        "for corpus in corpuses:\n",
        "    print(corpus)"
      ],
      "metadata": {
        "id": "9KTbAdg5Rms0",
        "outputId": "5a080b34-a339-43b6-d95c-b380563fa3a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "私 は 秋田 犬 が 大好き 。 秋田 犬 は 私 が 大好き 。\n",
            "私 は 犬 が 少し 苦手 。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CountVectorizerを使用する\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "1wMueIImSlnj"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
        "X = vectorizer.fit_transform(corpuses)\n",
        "print(vectorizer.get_feature_names_out(corpuses))"
      ],
      "metadata": {
        "id": "QdSJHk24UbkO",
        "outputId": "c41a800c-1618-4d80-8c7d-c640eaefd301",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['が' 'は' '大好き' '少し' '犬' '私' '秋田' '苦手']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.toarray())"
      ],
      "metadata": {
        "id": "v45TNqhXVQ_n",
        "outputId": "2621a6b3-9b64-444a-ad12-edbee85bb23d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2 2 2 0 2 2 2 0]\n",
            " [1 1 0 1 1 1 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bi-gramでの出現回数\n",
        "vectorizer2 = CountVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b', ngram_range=(2,2))\n",
        "X2 = vectorizer2.fit_transform(corpuses)\n",
        "print(f\"vectorizerのbi-gram:\\n{vectorizer2.get_feature_names_out(corpuses)}\")\n",
        "print(f\"vectorizerのbi-gramの出現回数:\\n{X2.toarray()}\")"
      ],
      "metadata": {
        "id": "sTQoWfyNVUa9",
        "outputId": "1dc0ff18-9790-44fe-b0f9-20345502efaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vectorizerのbi-gram:\n",
            "['が 大好き' 'が 少し' 'は 犬' 'は 私' 'は 秋田' '大好き 秋田' '少し 苦手' '犬 が' '犬 は' '私 が'\n",
            " '私 は' '秋田 犬']\n",
            "vectorizerのbi-gramの出現回数:\n",
            "[[2 0 0 1 1 1 0 1 1 1 1 2]\n",
            " [0 1 1 0 0 0 1 1 0 0 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF\n",
        "- TFとIDFの掛け算\n"
      ],
      "metadata": {
        "id": "CrWjUQOqpFyo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF (Term Frequency)\n",
        "- 単語出現頻度\n",
        "\n",
        "<方法>\n",
        "- 出現回数を利用する方法\n",
        "\n",
        "$$\n",
        "tf_{i, j} = n_{i, j} \\tag{1.1}\n",
        "$$\n",
        "単語 $w_{i}$の文書 $d_j$における出現回数\n",
        "\n",
        "- 出現回数の比率\n",
        "\n",
        "$$\n",
        "tf_{i, j} = \\cfrac{n_{i, j}}{\\sum_k n_{i, j}} \\tag{1.2}\n",
        "$$\n",
        "分子は、単語 $w_i$の文書 $d_j$における出現回数\n",
        "分母は、その単語の全ての文書中における出現回数の総和\n"
      ],
      "metadata": {
        "id": "clI4zqwgXAtP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IDF (Inverse Document Frequency)\n",
        "- 単語の逆文書頻度\n",
        "\n",
        "$$\n",
        "idf_{i, j} = \\log{\\cfrac{|D|}{|d:d \\owns w_i |}} \\tag{1.3}\n",
        "$$\n",
        "分子 $|D|$は、コーパス中における総文書数\n",
        "\n",
        "分母は、単語 $w_i$を含む文書数"
      ],
      "metadata": {
        "id": "NtQlYDJQXD3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TfidfVectorizerモジュールによるTF-IDFの文書ベクトル\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "sTpLk01lnNk8"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
        "X = vectorizer.fit_transform(corpuses)\n",
        "print(vectorizer.get_feature_names_out(corpuses))"
      ],
      "metadata": {
        "id": "66mGNr3Pa93q",
        "outputId": "a30a5736-cd32-463e-f388-31c24a47c51e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['が' 'は' '大好き' '少し' '犬' '私' '秋田' '苦手']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.toarray())"
      ],
      "metadata": {
        "id": "u1S1VeitblsG",
        "outputId": "dc22bd64-7f6b-4ecc-c9ee-f5d31c7aa264",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.35464863 0.35464863 0.49844628 0.         0.35464863 0.35464863\n",
            "  0.49844628 0.        ]\n",
            " [0.35464863 0.35464863 0.         0.49844628 0.35464863 0.35464863\n",
            "  0.         0.49844628]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# n-gramの場合\n",
        "vectorizer_ngram = TfidfVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b', ngram_range=(2, 2))\n",
        "X_ngram = vectorizer_ngram.fit_transform(corpuses)\n",
        "print(X_ngram.toarray())"
      ],
      "metadata": {
        "id": "_KAwCc41b4Pk",
        "outputId": "8d51749c-ca97-4705-fae3-2899ca77f85d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.53428425 0.         0.         0.26714212 0.26714212 0.26714212\n",
            "  0.         0.19007382 0.26714212 0.26714212 0.19007382 0.53428425]\n",
            " [0.         0.49922133 0.49922133 0.         0.         0.\n",
            "  0.49922133 0.35520009 0.         0.         0.35520009 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_X = vectorizer.transform(new_corpus)"
      ],
      "metadata": {
        "id": "cugADSn_dDKZ",
        "outputId": "d28aba40-12ac-4da1-cd16-74be4f81a2b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'new_corpus' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-a740298b0bec>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'new_corpus' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-_spBDNYeI-E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}