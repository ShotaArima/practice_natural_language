{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShotaArima/practice_natural_language/blob/main/chap.1-Document%20Vectorization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 第1章 文書のベクトル化"
      ],
      "metadata": {
        "id": "USEkWJ7hFUQF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## janomeによる単語分割"
      ],
      "metadata": {
        "id": "2EyHduCEFfJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install janome"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXNfbxBCEYvw",
        "outputId": "e3afcc7e-db67-480c-f417-cff84b49f9b6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting janome\n",
            "  Downloading Janome-0.5.0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Downloading Janome-0.5.0-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: janome\n",
            "Successfully installed janome-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import janome\n",
        "from janome.tokenizer import Tokenizer"
      ],
      "metadata": {
        "id": "l3kB5m4B1J0J"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = Tokenizer()"
      ],
      "metadata": {
        "id": "sZvdKVKiEM4I"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in t.tokenize('私は秋田犬が大好きです。'):\n",
        "  print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8bhuXfyFm0q",
        "outputId": "74fec763-4359-40ec-e728-de201e34578c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "私\t名詞,代名詞,一般,*,*,*,私,ワタシ,ワタシ\n",
            "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
            "秋田\t名詞,固有名詞,地域,一般,*,*,秋田,アキタ,アキタ\n",
            "犬\t名詞,一般,*,*,*,*,犬,イヌ,イヌ\n",
            "が\t助詞,格助詞,一般,*,*,*,が,ガ,ガ\n",
            "大好き\t名詞,形容動詞語幹,*,*,*,*,大好き,ダイスキ,ダイスキ\n",
            "です\t助動詞,*,*,*,特殊・デス,基本形,です,デス,デス\n",
            "。\t記号,句点,*,*,*,*,。,。,。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# wakati=Trueで分かち書きモード\n",
        "for token in t.tokenize('私は秋田犬が大好きです。', wakati=True):\n",
        "  print(token, end='/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpIEgY79F19i",
        "outputId": "f4b51253-af21-485c-8640-aba23f59975d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "私/は/秋田/犬/が/大好き/です/。/"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 半角スペースで分割\n",
        "for token in t.tokenize('私は秋田犬が大好きです。', wakati=True):\n",
        "  print(token, end=' ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oH0oxKabGtid",
        "outputId": "e271052f-8a6d-4e19-f486-2bab2ba0af72"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "私 は 秋田 犬 が 大好き です 。 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# リストに格納\n",
        "words = [token for token in t.tokenize('私は秋田犬が好きです。', wakati=True)]\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pu_QQBgHU2y",
        "outputId": "1531d766-e10d-47d5-f1a3-1027fb688400"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['私', 'は', '秋田', '犬', 'が', '好き', 'です', '。']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## N-gram"
      ],
      "metadata": {
        "id": "-UVxhpBNPDNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# bi-gram\n",
        "for i in range (len(words)-1):\n",
        "    print(f\"{i}:{words[i:i+2]}\")"
      ],
      "metadata": {
        "id": "3fBbBogOH01a",
        "outputId": "66802e31-bc47-4bc9-87f2-af764bd7f3f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:['私', 'は']\n",
            "1:['は', '秋田']\n",
            "2:['秋田', '犬']\n",
            "3:['犬', 'が']\n",
            "4:['が', '好き']\n",
            "5:['好き', 'です']\n",
            "6:['です', '。']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# n-gram\n",
        "def get_word_n_grams(sentence, n):\n",
        "    words = [token for token in t.tokenize(sentence, wakati=True)]\n",
        "    result = []\n",
        "    for index in range(len(words)):\n",
        "        result.append(words[index: index+n])\n",
        "        if index+n >= len(words):\n",
        "            return result"
      ],
      "metadata": {
        "id": "BpT7FtHwPMT6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = 'すもももももももものうち'\n",
        "print(get_word_n_grams(input, 3))"
      ],
      "metadata": {
        "id": "GyUk0jJzQh5z",
        "outputId": "aeb2617d-4af5-479b-ba9f-880ccfa86f17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['すもも', 'も', 'もも'], ['も', 'もも', 'も'], ['もも', 'も', 'もも'], ['も', 'もも', 'の'], ['もも', 'の', 'うち']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 文字のN-gram"
      ],
      "metadata": {
        "id": "a1zRT4x0RvcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_charactor_n_gram(sentence, n):\n",
        "    result = []\n",
        "    for index in range(len(sentence)):\n",
        "        result.append(sentence[index: index+n])\n",
        "        if index+n >= len(sentence):\n",
        "            return result"
      ],
      "metadata": {
        "id": "MV9K0UYJQvLs"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input2 = '私は秋田犬が大好きだ。'\n",
        "print(get_charactor_n_gram(input2, 2))"
      ],
      "metadata": {
        "id": "8j3A4EYKSJSU",
        "outputId": "49dbbf1b-8abe-417d-b23b-7660bf4116f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['私は', 'は秋', '秋田', '田犬', '犬が', 'が大', '大好', '好き', 'きだ', 'だ。']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag-of-words"
      ],
      "metadata": {
        "id": "FBXV9I0jnH5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(sentence):\n",
        "    return [token for token in t.tokenize(sentence, wakati=True)]"
      ],
      "metadata": {
        "id": "Lwr1276ESXwk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words1 = '私は秋田犬が大好き。'\n",
        "words2 = '私は犬が少し苦手。'\n",
        "\n",
        "print(f\"words1:{tokenize(words1)}\")\n",
        "print(f\"words2:{tokenize(words2)}\")"
      ],
      "metadata": {
        "id": "aUH18EaMnUef",
        "outputId": "539769d7-cc90-44fd-ae62-260929e6c6dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "words1:['私', 'は', '秋田', '犬', 'が', '大好き', '。']\n",
            "words2:['私', 'は', '犬', 'が', '少し', '苦手', '。']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 文書ベクトルの作成\n",
        "def get_bag_of_words(document):\n",
        "    result_dict = {}\n",
        "    words = tokenize(document)\n",
        "    for word in words:\n",
        "        if word not in result_dict:\n",
        "            result_dict[word] = 1\n",
        "        else:\n",
        "            result_dict[word] += 1\n",
        "    return result_dict"
      ],
      "metadata": {
        "id": "FUv5S_J2ntwo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document1 = '私は秋田犬が大好き。秋田犬は私が大好き。'\n",
        "document2 = '私は犬が少し苦手。'\n",
        "print(f\"document1:{get_bag_of_words(document1)}\")\n",
        "print(f\"document2:{get_bag_of_words(document2)}\")"
      ],
      "metadata": {
        "id": "y3WKy5ZEppGb",
        "outputId": "5ab699ba-6417-4d40-b7e4-804a80d6bb39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "document1:{'私': 2, 'は': 2, '秋田': 2, '犬': 2, 'が': 2, '大好き': 2, '。': 2}\n",
            "document2:{'私': 1, 'は': 1, '犬': 1, 'が': 1, '少し': 1, '苦手': 1, '。': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 辞書\n",
        "def make_dictinary(documents):\n",
        "    result_dict = {}\n",
        "    index = 1\n",
        "    for doc in documents:\n",
        "        words = tokenize(doc)\n",
        "        for word in words:\n",
        "            if word not in result_dict:\n",
        "                result_dict[word] = index\n",
        "                index+=1\n",
        "    return result_dict"
      ],
      "metadata": {
        "id": "0P316AEHp_I5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [document1, document2]\n",
        "dictionary = make_dictinary(documents)\n",
        "print(f\"dict:{dict}\")"
      ],
      "metadata": {
        "id": "jFeG0ZEisI14",
        "outputId": "3ba21543-1aab-461b-cfc4-e5ccd32e1b30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict:<class 'dict'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 用例ベクトルの作成\n",
        "def make_BOW_vectors(documents, dictionary):\n",
        "    result_vectors = []\n",
        "    for doc in documents:\n",
        "        vec = {}\n",
        "        words = tokenize(doc)\n",
        "        for entry in dictionary:\n",
        "            vec[dictionary[entry]]=0\n",
        "        for word in words:\n",
        "            vec[dictionary[word]] += 1\n",
        "        result_vectors.append(vec)\n",
        "    return result_vectors"
      ],
      "metadata": {
        "id": "xu71RG3psaZh"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectors = make_BOW_vectors(documents, dictionary)\n",
        "print(f\"結果:{vectors}\")\n",
        "\n",
        "id_to_word = {v: k for k, v in dictionary.items()} # 辞書型を逆変換する\n",
        "word_occurrences = []\n",
        "for occurrence in vectors:\n",
        "    word_dict = {}\n",
        "    for id_num, count in occurrence.items():\n",
        "        word = id_to_word[id_num]\n",
        "        word_dict[word] = count\n",
        "    word_occurrences.append(word_dict)\n",
        "for occurrences in word_occurrences:\n",
        "    print(f\"結果:{occurrences}\")"
      ],
      "metadata": {
        "id": "k50KEnwMtufs",
        "outputId": "09b841e7-7a05-447c-dca9-71d360a9a3fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "結果:[{1: 2, 2: 2, 3: 2, 4: 2, 5: 2, 6: 2, 7: 2, 8: 0, 9: 0}, {1: 1, 2: 1, 3: 0, 4: 1, 5: 1, 6: 0, 7: 1, 8: 1, 9: 1}]\n",
            "結果:{'私': 2, 'は': 2, '秋田': 2, '犬': 2, 'が': 2, '大好き': 2, '。': 2, '少し': 0, '苦手': 0}\n",
            "結果:{'私': 1, 'は': 1, '秋田': 0, '犬': 1, 'が': 1, '大好き': 0, '。': 1, '少し': 1, '苦手': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CountVectorizerモジュール\n",
        "- scikit-learnのモジュール"
      ],
      "metadata": {
        "id": "K2FIpU4ZQqQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# wakatiを使用して単語間に半角スペースを入れる\n",
        "def make_corpus(documents):\n",
        "    result_corps = []\n",
        "    for doc in documents:\n",
        "        words = tokenize(doc)\n",
        "        text=\" \".join(words)\n",
        "        result_corps.append(text)\n",
        "    return result_corps"
      ],
      "metadata": {
        "id": "B-Ymke3quyuK"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t  = Tokenizer()\n",
        "document1 = '私は秋田犬が大好き。秋田犬は私が大好き。'\n",
        "document2 = '私は犬が少し苦手。'\n",
        "documents = [document1, document2]\n",
        "corpuses = make_corps(documents)\n",
        "for corpus in corpuses:\n",
        "    print(corpus)"
      ],
      "metadata": {
        "id": "9KTbAdg5Rms0",
        "outputId": "0f6a0356-66b4-4a31-ba39-0c001870f039",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "私 は 秋田 犬 が 大好き 。 秋田 犬 は 私 が 大好き 。\n",
            "私 は 犬 が 少し 苦手 。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CountVectorizerを使用する\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "1wMueIImSlnj"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
        "X = vectorizer.fit_transform(corpuses)\n",
        "print(vectorizer.get_feature_names_out(corpuses))"
      ],
      "metadata": {
        "id": "QdSJHk24UbkO",
        "outputId": "6149f1b0-7a51-4c0b-df58-b9e667e7a00c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['が' 'は' '大好き' '少し' '犬' '私' '秋田' '苦手']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.toarray())"
      ],
      "metadata": {
        "id": "v45TNqhXVQ_n",
        "outputId": "60ed87c0-5ba3-499f-ada9-8cfb231acdcf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2 2 2 0 2 2 2 0]\n",
            " [1 1 0 1 1 1 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bi-gramでの出現回数\n",
        "vectorizer2 = CountVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b', ngram_range=(2,2))\n",
        "X2 = vectorizer2.fit_transform(corpuses)\n",
        "print(f\"vectorizerのbi-gram:\\n{vectorizer2.get_feature_names_out(corpuses)}\")\n",
        "print(f\"vectorizerのbi-gramの出現回数:\\n{X2.toarray()}\")"
      ],
      "metadata": {
        "id": "sTQoWfyNVUa9",
        "outputId": "3cd6bcee-079a-4131-ce54-cf23010746d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vectorizerのbi-gram:\n",
            "['が 大好き' 'が 少し' 'は 犬' 'は 私' 'は 秋田' '大好き 秋田' '少し 苦手' '犬 が' '犬 は' '私 が'\n",
            " '私 は' '秋田 犬']\n",
            "vectorizerのbi-gramの出現回数:\n",
            "[[2 0 0 1 1 1 0 1 1 1 1 2]\n",
            " [0 1 1 0 0 0 1 1 0 0 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF\n",
        "- TFとIDFの掛け算\n"
      ],
      "metadata": {
        "id": "CrWjUQOqpFyo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF (Term Frequency)\n",
        "- 単語出現頻度\n",
        "\n",
        "<方法>\n",
        "- 出現回数を利用する方法\n",
        "\n",
        "$$\n",
        "tf_{i, j} = n_{i, j} \\tag{1.1}\n",
        "$$\n",
        "単語 $w_{i}$の文書 $d_j$における出現回数\n",
        "\n",
        "- 出現回数の比率\n",
        "\n",
        "$$\n",
        "tf_{i, j} = \\cfrac{n_{i, j}}{\\sum_k n_{i, j}} \\tag{1.2}\n",
        "$$\n",
        "分子は、単語 $w_i$の文書 $d_j$における出現回数\n",
        "分母は、その単語の全ての文書中における出現回数の総和\n"
      ],
      "metadata": {
        "id": "clI4zqwgXAtP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IDF (Inverse Document Frequency)\n",
        "- 単語の逆文書頻度\n",
        "\n",
        "$$\n",
        "idf_{i, j} = \\log{\\cfrac{|D|}{|d:d \\owns w_i |}} \\tag{1.3}\n",
        "$$\n",
        "分子 $|D|$は、コーパス中における総文書数\n",
        "\n",
        "分母は、単語 $w_i$を含む文書数"
      ],
      "metadata": {
        "id": "NtQlYDJQXD3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TfidfVectorizerモジュールによるTF-IDFの文書ベクトル\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "sTpLk01lnNk8"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
        "X = vectorizer.fit_transform(corpuses)\n",
        "print(vectorizer.get_feature_names_out(corpuses))"
      ],
      "metadata": {
        "id": "66mGNr3Pa93q",
        "outputId": "48ad53ea-d0af-46a7-d795-d604ec31491d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['が' 'は' '大好き' '少し' '犬' '私' '秋田' '苦手']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.toarray())"
      ],
      "metadata": {
        "id": "u1S1VeitblsG",
        "outputId": "9191c341-2cb5-4c40-c295-507be68f5d49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.35464863 0.35464863 0.49844628 0.         0.35464863 0.35464863\n",
            "  0.49844628 0.        ]\n",
            " [0.35464863 0.35464863 0.         0.49844628 0.35464863 0.35464863\n",
            "  0.         0.49844628]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# n-gramの場合\n",
        "vectorizer_ngram = TfidfVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b', ngram_range=(2, 2))\n",
        "X_ngram = vectorizer_ngram.fit_transform(corpuses)\n",
        "print(X_ngram.toarray())"
      ],
      "metadata": {
        "id": "_KAwCc41b4Pk",
        "outputId": "5d360797-786d-491a-e665-b59568b4928f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.53428425 0.         0.         0.26714212 0.26714212 0.26714212\n",
            "  0.         0.19007382 0.26714212 0.26714212 0.19007382 0.53428425]\n",
            " [0.         0.49922133 0.49922133 0.         0.         0.\n",
            "  0.49922133 0.35520009 0.         0.         0.35520009 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Latent Semantic Analysis\n",
        "用例ベクトルによって文書分類を行っていくが、その中で特異値分解を行うことでよりそのベクトルが圧縮され、ノイズのあるデータでも問題なく使用することができるようになる"
      ],
      "metadata": {
        "id": "9LmUjEcugGaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document1='私は秋田犬が大好き。秋田犬は私が大好き。'\n",
        "document2='私は犬が少し苦手。'\n",
        "document3='私はぶたとペンギンが好きです。ペンギンは鳥の仲間ですが、水族館で見ることができます。'\n",
        "document4='隣の家の犬はよく吠える犬です。'\n",
        "document5='ゴロピカドンが好きです。'\n",
        "document6='事務室のカギが見つからないと思ったが、よく探したらあった。'\n",
        "document7='フルーツはどれも大好きですが、私の好物はイチゴです。'\n",
        "document8='りんごもミカンもパイナップルも、どれもそれぞれとても美味しい果物です。'\n",
        "document9='私はフルーツが好きで、よくフルーツパーラーに食べに行きます。'\n",
        "document10='ぶどうとメロンは必需品。'\n",
        "documents=[document1, document2,document3, document4, document5, document6, document7, document8, document9, document10]\n",
        "\n",
        "corpus=make_corpus(documents)\n",
        "print(corpus)"
      ],
      "metadata": {
        "id": "-_spBDNYeI-E",
        "outputId": "8e7ebc17-94ba-49bf-d5a9-1bfbb151815d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['私 は 秋田 犬 が 大好き 。 秋田 犬 は 私 が 大好き 。', '私 は 犬 が 少し 苦手 。', '私 は ぶた と ペンギン が 好き です 。 ペンギン は 鳥 の 仲間 です が 、 水族館 で 見る こと が でき ます 。', '隣 の 家 の 犬 は よく 吠える 犬 です 。', 'ゴロピカドン が 好き です 。', '事務 室 の カギ が 見つから ない と 思っ た が 、 よく 探し たら あっ た 。', 'フルーツ は どれ も 大好き です が 、 私 の 好物 は イチゴ です 。', 'りんご も ミカン も パイナップル も 、 どれ も それぞれ とても 美味しい 果物 です 。', '私 は フルーツ が 好き で 、 よく フルーツパーラー に 食べ に 行き ます 。', 'ぶどう と メロン は 必需 品 。']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b', sublinear_tf=True)\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(X.shape)"
      ],
      "metadata": {
        "id": "FR_TzWwDe45f",
        "outputId": "b324c6a1-233b-4af6-bae9-c6a10aec1c41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 57)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J-scfj2ThMs0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}