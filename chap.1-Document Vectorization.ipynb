{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShotaArima/practice_natural_language/blob/main/chap.1-Document%20Vectorization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 第1章 文書のベクトル化"
      ],
      "metadata": {
        "id": "USEkWJ7hFUQF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## janomeによる単語分割"
      ],
      "metadata": {
        "id": "2EyHduCEFfJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install janome"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXNfbxBCEYvw",
        "outputId": "7690c517-1d89-42e5-a25a-4da62730a6c7"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: janome in /usr/local/lib/python3.10/dist-packages (0.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import janome\n",
        "from janome.tokenizer import Tokenizer"
      ],
      "metadata": {
        "id": "l3kB5m4B1J0J"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = Tokenizer()"
      ],
      "metadata": {
        "id": "sZvdKVKiEM4I"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in t.tokenize('私は秋田犬が大好きです。'):\n",
        "  print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8bhuXfyFm0q",
        "outputId": "86a03b7a-e142-453b-f409-ea8416ca8121"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "私\t名詞,代名詞,一般,*,*,*,私,ワタシ,ワタシ\n",
            "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
            "秋田\t名詞,固有名詞,地域,一般,*,*,秋田,アキタ,アキタ\n",
            "犬\t名詞,一般,*,*,*,*,犬,イヌ,イヌ\n",
            "が\t助詞,格助詞,一般,*,*,*,が,ガ,ガ\n",
            "大好き\t名詞,形容動詞語幹,*,*,*,*,大好き,ダイスキ,ダイスキ\n",
            "です\t助動詞,*,*,*,特殊・デス,基本形,です,デス,デス\n",
            "。\t記号,句点,*,*,*,*,。,。,。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# wakati=Trueで分かち書きモード\n",
        "for token in t.tokenize('私は秋田犬が大好きです。', wakati=True):\n",
        "  print(token, end='/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpIEgY79F19i",
        "outputId": "ab0a7161-2b42-4a86-9e3f-221aef5235dc"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "私/は/秋田/犬/が/大好き/です/。/"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 半角スペースで分割\n",
        "for token in t.tokenize('私は秋田犬が大好きです。', wakati=True):\n",
        "  print(token, end=' ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oH0oxKabGtid",
        "outputId": "340bdb90-abe8-402f-f3f8-6332111414ef"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "私 は 秋田 犬 が 大好き です 。 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# リストに格納\n",
        "words = [token for token in t.tokenize('私は秋田犬が好きです。', wakati=True)]\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pu_QQBgHU2y",
        "outputId": "ae3ed223-341c-40bb-cfa6-78a9ea9cdab2"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['私', 'は', '秋田', '犬', 'が', '好き', 'です', '。']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## N-gram"
      ],
      "metadata": {
        "id": "-UVxhpBNPDNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# bi-gram\n",
        "for i in range (len(words)-1):\n",
        "    print(f\"{i}:{words[i:i+2]}\")"
      ],
      "metadata": {
        "id": "3fBbBogOH01a",
        "outputId": "30594b3a-b20c-4ab3-a2e6-902c4c86c9c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:['私', 'は']\n",
            "1:['は', '秋田']\n",
            "2:['秋田', '犬']\n",
            "3:['犬', 'が']\n",
            "4:['が', '好き']\n",
            "5:['好き', 'です']\n",
            "6:['です', '。']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# n-gram\n",
        "def get_word_n_grams(sentence, n):\n",
        "    words = [token for token in t.tokenize(sentence, wakati=True)]\n",
        "    result = []\n",
        "    for index in range(len(words)):\n",
        "        result.append(words[index: index+n])\n",
        "        if index+n >= len(words):\n",
        "            return result"
      ],
      "metadata": {
        "id": "BpT7FtHwPMT6"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = 'すもももももももものうち'\n",
        "print(get_word_n_grams(input, 3))"
      ],
      "metadata": {
        "id": "GyUk0jJzQh5z",
        "outputId": "73e677eb-5a2f-4806-9f19-ef91b82fe517",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['すもも', 'も', 'もも'], ['も', 'もも', 'も'], ['もも', 'も', 'もも'], ['も', 'もも', 'の'], ['もも', 'の', 'うち']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 文字のN-gram"
      ],
      "metadata": {
        "id": "a1zRT4x0RvcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_charactor_n_gram(sentence, n):\n",
        "    result = []\n",
        "    for index in range(len(sentence)):\n",
        "        result.append(sentence[index: index+n])\n",
        "        if index+n >= len(sentence):\n",
        "            return result"
      ],
      "metadata": {
        "id": "MV9K0UYJQvLs"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input2 = '私は秋田犬が大好きだ。'\n",
        "print(get_charactor_n_gram(input2, 2))"
      ],
      "metadata": {
        "id": "8j3A4EYKSJSU",
        "outputId": "77799cab-82e2-43ae-8e51-2292fd3bd024",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['私は', 'は秋', '秋田', '田犬', '犬が', 'が大', '大好', '好き', 'きだ', 'だ。']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag-of-words"
      ],
      "metadata": {
        "id": "FBXV9I0jnH5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(sentence):\n",
        "    return [token for token in t.tokenize(sentence, wakati=True)]"
      ],
      "metadata": {
        "id": "Lwr1276ESXwk"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words1 = '私は秋田犬が大好き。'\n",
        "words2 = '私は犬が少し苦手。'\n",
        "\n",
        "print(f\"words1:{tokenize(words1)}\")\n",
        "print(f\"words2:{tokenize(words2)}\")"
      ],
      "metadata": {
        "id": "aUH18EaMnUef",
        "outputId": "fc44a235-4da9-4d81-fe6c-ec5a56d569af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "words1:['私', 'は', '秋田', '犬', 'が', '大好き', '。']\n",
            "words2:['私', 'は', '犬', 'が', '少し', '苦手', '。']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 文書ベクトルの作成\n",
        "def get_bag_of_words(document):\n",
        "    result_dict = {}\n",
        "    words = tokenize(document)\n",
        "    for word in words:\n",
        "        if word not in result_dict:\n",
        "            result_dict[word] = 1\n",
        "        else:\n",
        "            result_dict[word] += 1\n",
        "    return result_dict"
      ],
      "metadata": {
        "id": "FUv5S_J2ntwo"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document1 = '私は秋田犬が大好き。秋田犬は私が大好き。'\n",
        "document2 = '私は犬が少し苦手。'\n",
        "print(f\"document1:{get_bag_of_words(document1)}\")\n",
        "print(f\"document2:{get_bag_of_words(document2)}\")"
      ],
      "metadata": {
        "id": "y3WKy5ZEppGb",
        "outputId": "b2287422-b2b4-4045-f037-cabf48577608",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "document1:{'私': 2, 'は': 2, '秋田': 2, '犬': 2, 'が': 2, '大好き': 2, '。': 2}\n",
            "document2:{'私': 1, 'は': 1, '犬': 1, 'が': 1, '少し': 1, '苦手': 1, '。': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 辞書\n",
        "def make_dictinary(documents):\n",
        "    result_dict = {}\n",
        "    index = 1\n",
        "    for doc in documents:\n",
        "        words = tokenize(doc)\n",
        "        for word in words:\n",
        "            if word not in result_dict:\n",
        "                result_dict[word] = index\n",
        "                index+=1\n",
        "    return result_dict"
      ],
      "metadata": {
        "id": "0P316AEHp_I5"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [document1, document2]\n",
        "dictionary = make_dictinary(documents)\n",
        "print(f\"dict:{dict}\")"
      ],
      "metadata": {
        "id": "jFeG0ZEisI14",
        "outputId": "8df8321e-9d65-4258-e95a-ff2a272c42cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict:<class 'dict'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 用例ベクトルの作成\n",
        "def make_BOW_vectors(documents, dictionary):\n",
        "    result_vectors = []\n",
        "    for doc in documents:\n",
        "        vec = {}\n",
        "        words = tokenize(doc)\n",
        "        for entry in dictionary:\n",
        "            vec[dictionary[entry]]=0\n",
        "        for word in words:\n",
        "            vec[dictionary[word]] += 1\n",
        "        result_vectors.append(vec)\n",
        "    return result_vectors"
      ],
      "metadata": {
        "id": "xu71RG3psaZh"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectors = make_BOW_vectors(documents, dictionary)\n",
        "print(f\"結果:{vectors}\")\n",
        "\n",
        "id_to_word = {v: k for k, v in dictionary.items()} # 辞書型を逆変換する\n",
        "word_occurrences = []\n",
        "for occurrence in vectors:\n",
        "    word_dict = {}\n",
        "    for id_num, count in occurrence.items():\n",
        "        word = id_to_word[id_num]\n",
        "        word_dict[word] = count\n",
        "    word_occurrences.append(word_dict)\n",
        "for occurrences in word_occurrences:\n",
        "    print(f\"結果:{occurrences}\")"
      ],
      "metadata": {
        "id": "k50KEnwMtufs",
        "outputId": "f06a3197-9f65-422b-91cb-b73a127cb260",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "結果:[{1: 2, 2: 2, 3: 2, 4: 2, 5: 2, 6: 2, 7: 2, 8: 0, 9: 0}, {1: 1, 2: 1, 3: 0, 4: 1, 5: 1, 6: 0, 7: 1, 8: 1, 9: 1}]\n",
            "結果:{'私': 2, 'は': 2, '秋田': 2, '犬': 2, 'が': 2, '大好き': 2, '。': 2, '少し': 0, '苦手': 0}\n",
            "結果:{'私': 1, 'は': 1, '秋田': 0, '犬': 1, 'が': 1, '大好き': 0, '。': 1, '少し': 1, '苦手': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CountVectorizerモジュール\n",
        "- scikit-learnのモジュール"
      ],
      "metadata": {
        "id": "K2FIpU4ZQqQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# wakatiを使用して単語間に半角スペースを入れる\n",
        "def make_corps(documents):\n",
        "    result_corps = []\n",
        "    for doc in documents:\n",
        "        words = tokenize(doc)\n",
        "        text=\" \".join(words)\n",
        "        result_corps.append(text)\n",
        "    return result_corps"
      ],
      "metadata": {
        "id": "B-Ymke3quyuK"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t  = Tokenizer()\n",
        "document1 = '私は秋田犬が大好き。秋田犬は私が大好き。'\n",
        "document2 = '私は犬が少し苦手。'\n",
        "documents = [document1, document2]\n",
        "corpuses = make_corps(documents)\n",
        "for corpus in corpuses:\n",
        "    print(corpus)"
      ],
      "metadata": {
        "id": "9KTbAdg5Rms0",
        "outputId": "daa38513-10a6-40fd-b849-13727516fe5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "私 は 秋田 犬 が 大好き 。 秋田 犬 は 私 が 大好き 。\n",
            "私 は 犬 が 少し 苦手 。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CountVectorizerを使用する\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "1wMueIImSlnj"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
        "X = vectorizer.fit_transform(corpuses)\n",
        "print(vectorizer.get_feature_names_out(corpuses))"
      ],
      "metadata": {
        "id": "QdSJHk24UbkO",
        "outputId": "5d4fe1b0-b964-413d-b919-0645d049a0ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['が' 'は' '大好き' '少し' '犬' '私' '秋田' '苦手']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.toarray())"
      ],
      "metadata": {
        "id": "v45TNqhXVQ_n",
        "outputId": "406c0c74-b902-42db-97cc-5211e0273c5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2 2 2 0 2 2 2 0]\n",
            " [1 1 0 1 1 1 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bi-gramでの出現回数\n",
        "vectorizer2 = CountVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b', ngram_range=(2,2))\n",
        "X2 = vectorizer2.fit_transform(corpuses)\n",
        "print(f\"vectorizerのbi-gram:\\n{vectorizer2.get_feature_names_out(corpuses)}\")\n",
        "print(f\"vectorizerのbi-gramの出現回数:\\n{X2.toarray()}\")"
      ],
      "metadata": {
        "id": "sTQoWfyNVUa9",
        "outputId": "ec1716a7-5453-4786-85ea-5f0698f22a3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vectorizerのbi-gram:\n",
            "['が 大好き' 'が 少し' 'は 犬' 'は 私' 'は 秋田' '大好き 秋田' '少し 苦手' '犬 が' '犬 は' '私 が'\n",
            " '私 は' '秋田 犬']\n",
            "vectorizerのbi-gramの出現回数:\n",
            "[[2 0 0 1 1 1 0 1 1 1 1 2]\n",
            " [0 1 1 0 0 0 1 1 0 0 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF\n",
        "- TFとIDFの掛け算\n"
      ],
      "metadata": {
        "id": "CrWjUQOqpFyo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF (Term Frequency)\n",
        "- 単語出現頻度\n",
        "\n",
        "<方法>\n",
        "- 出現回数を利用する方法\n",
        "\n",
        "$$\n",
        "tf_{i, j} = n_{i, j} \\tag{1.1}\n",
        "$$\n",
        "単語 $w_{i}$の文書 $d_j$における出現回数\n",
        "\n",
        "- 出現回数の比率\n",
        "\n",
        "$$\n",
        "tf_{i, j} = \\cfrac{n_{i, j}}{\\sum_k n_{i, j}} \\tag{1.2}\n",
        "$$\n",
        "分子は、単語 $w_i$の文書 $d_j$における出現回数\n",
        "分母は、その単語の全ての文書中における出現回数の総和\n"
      ],
      "metadata": {
        "id": "clI4zqwgXAtP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IDF (Inverse Document Frequency)\n",
        "- 単語の逆文書頻度\n",
        "\n",
        "$$\n",
        "idf_{i, j} = \\log{\\cfrac{|D|}{|d:d \\owns w_i |}} \\tag{1.3}\n",
        "$$\n",
        "分子 $|D|$は、コーパス中における総文書数\n",
        "\n",
        "分母は、単語 $w_i$を含む文書数"
      ],
      "metadata": {
        "id": "NtQlYDJQXD3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TfidfVectorizerモジュールによるTF-IDFの文書ベクトル\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "sTpLk01lnNk8"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(token_pattern=;(?u))"
      ],
      "metadata": {
        "id": "66mGNr3Pa93q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}